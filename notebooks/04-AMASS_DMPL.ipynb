{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Body Data\n",
    "To better capture soft-tissue dynamics, SMPL offers a shape dependent model of\n",
    "dynamic deformations adapted from the  Dyna model [Pons-Moll et al. 2015].\n",
    "We call dynamic blend shapes as DMPL. DMPL is obtained by applying PCA on vertex errors between\n",
    "SMPL and Dyna training meshes, transformed into the rest pose.\n",
    "Animating soft-tissue dynamics in a standard rendering engine simply requires using the dynamic linear\n",
    "blend shape coefficients similar to any other time-dependent parameters of SMPL, e.g. pose and global translation.\n",
    "AMASS uses 8 DMPL parameters to extract soft-tissue motions realistically from a sparse set of markers.\n",
    "\n",
    "DMPLs are most visible for rapid motions on fatty tissue.\n",
    "\n",
    "Bellow we visualize a sequence form AMASS with and without DMPL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Preparing the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "from os import path as osp\n",
    "\n",
    "support_dir = '../support_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "We assume you have downloaded the required body model and placed them in body_models directory of this repository.\n",
    "For SMPL+H body model, download it from http://mano.is.tue.mpg.de/.\n",
    "Please download the AMASS version of the model with DMPL blendshapes.\n",
    "You can obtain dynamic shape blendshapes, e.g. DMPLs, from http://smpl.is.tue.mpg.de.\n",
    "If you use any of these models in your research please follow their respective citation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys available:['trans', 'gender', 'mocap_framerate', 'betas', 'dmpls', 'poses']\n",
      "The subject of the mocap sequence is  female.\n"
     ]
    }
   ],
   "source": [
    "amass_npz_fname = osp.join(support_dir, 'github_data/dmpl_sample.npz') # the path to body data\n",
    "bdata = np.load(amass_npz_fname)\n",
    "\n",
    "# you can set the gender manually and if it differs from data's then contact or \n",
    "# interpenetration issues might happen\n",
    "subject_gender = (bdata['gender'].tolist()).decode()\n",
    "\n",
    "print('Data keys available:%s'%list(bdata.keys()))\n",
    "\n",
    "print('The subject of the mocap sequence is  {}.'.format(subject_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm_fname = osp.join(support_dir, 'body_models/smplh/{}/model.npz'.format(subject_gender))\n",
    "dmpl_fname = osp.join(support_dir, 'body_models/dmpls/{}/model.npz'.format(subject_gender))\n",
    "\n",
    "num_betas = 16 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname, num_betas=num_betas, num_dmpls=num_dmpls, dmpl_fname=dmpl_fname).to(comp_device)\n",
    "faces = c2c(bm.f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "The provided sample data has the original mocap marker data.\n",
    "In the following we make PyTorch tensors for parameters controlling different part of the body model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body parameter vector shapes: \n",
      "root_orient: torch.Size([235, 3]) \n",
      "pose_body: torch.Size([235, 63]) \n",
      "pose_hand: torch.Size([235, 90]) \n",
      "trans: torch.Size([235, 3]) \n",
      "betas: torch.Size([235, 16]) \n",
      "dmpls: torch.Size([235, 8])\n",
      "time_length = 235\n"
     ]
    }
   ],
   "source": [
    "time_length = len(bdata['trans'])\n",
    "\n",
    "body_parms = {\n",
    "    'root_orient': torch.Tensor(bdata['poses'][:, :3]).to(comp_device), # controls the global root orientation\n",
    "    'pose_body': torch.Tensor(bdata['poses'][:, 3:66]).to(comp_device), # controls the body\n",
    "    'pose_hand': torch.Tensor(bdata['poses'][:, 66:]).to(comp_device), # controls the finger articulation\n",
    "    'trans': torch.Tensor(bdata['trans']).to(comp_device), # controls the global body position\n",
    "    'betas': torch.Tensor(np.repeat(bdata['betas'][:num_betas][np.newaxis], repeats=time_length, axis=0)).to(comp_device), # controls the body shape. Body shape is static\n",
    "    'dmpls': torch.Tensor(bdata['dmpls'][:, :num_dmpls]).to(comp_device) # controls soft tissue dynamics\n",
    "}\n",
    "\n",
    "print('Body parameter vector shapes: \\n{}'.format(' \\n'.join(['{}: {}'.format(k,v.shape) for k,v in body_parms.items()])))\n",
    "print('time_length = {}'.format(time_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the required files for viewing out mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from body_visualizer.tools.vis_tools import colors\n",
    "from body_visualizer.mesh.mesh_viewer import MeshViewer\n",
    "from body_visualizer.mesh.sphere import points_to_spheres\n",
    "from body_visualizer.tools.vis_tools import show_image\n",
    "from body_visualizer.tools.vis_tools import imagearray2file\n",
    "\n",
    "imw, imh=1600, 1600\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize DMPLs\n",
    "\n",
    "You can control the soft tissue dynamics with DMPL parameters.\n",
    "Please have in mind, to better visualize DMPLs you would need to render a sequence.\n",
    "Refer to full renders of the parameter sequences in our [website](https://amass.is.tue.mpg.de/).\n",
    "\n",
    "Below we add an offset value to body with and withou DMPL paramters for visualization.\n",
    "\n",
    "Body on the left is with DMPLs and on the right without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "w_dmpl_parms = {k:v for k,v in body_parms.items() if k in ['pose_body', 'betas', 'pose_hand', 'dmpls']}\n",
    "w_dmpl_trans = torch.zeros_like(body_parms['trans'])\n",
    "w_dmpl_trans[:,0] += -0.5\n",
    "w_dmpl_parms['trans'] = w_dmpl_trans\n",
    "body_dmpls = bm(**w_dmpl_parms)\n",
    "\n",
    "wo_dmpl_parms = {k:v for k,v in body_parms.items() if k in ['pose_body', 'betas', 'pose_hand']}\n",
    "wo_dmpl_trans = torch.zeros_like(body_parms['trans'])\n",
    "wo_dmpl_trans[:,0] += 0.5\n",
    "wo_dmpl_parms['trans'] = wo_dmpl_trans\n",
    "body_wo_dmpls = bm(**wo_dmpl_parms)\n",
    "\n",
    "def vis_dmpl_comparision():\n",
    "    image_arr = []\n",
    "    for fId in range(time_length):\n",
    "        body_mesh_w_dmpl = trimesh.Trimesh(vertices=c2c(body_dmpls.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "        body_mesh_wo_dmpl = trimesh.Trimesh(vertices=c2c(body_wo_dmpls.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    \n",
    "        mv.set_static_meshes([body_mesh_w_dmpl, body_mesh_wo_dmpl])\n",
    "        body_image = mv.render(render_wireframe=False)\n",
    "        image_arr.append(body_image)\n",
    "\n",
    "    image_arr = np.array(image_arr).reshape([1,1,-1, 1600, 1600, 3])\n",
    "    imagearray2file(image_arr, out_fname='dmpl_sample.gif', fps=60)\n",
    "    imagearray2file(image_arr, out_fname='dmpl_sample.mp4', fps=60)\n",
    "\n",
    "    \n",
    "vis_dmpl_comparision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dmpl_sample.gif\" width=\"750\" align=\"center\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}